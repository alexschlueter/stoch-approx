\documentclass[ngerman,a4paper,11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{babel}
\usepackage{csquotes}
\usepackage{xfrac}
\usepackage{bbm}
\usepackage{braket}
\usepackage{enumitem}
\usepackage[shortcuts]{extdash}
\usepackage{cancel}
\usepackage[backend=biber,style=alphabetic]{biblatex}
\usepackage{nameref}
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage[capitalize]{cleveref}

\bibliography{literatur.bib}

\newlist{thmlist}{enumerate}{1}
\setlist[thmlist]{label=(\roman{thmlisti}), ref=\thethm (\roman{thmlisti}),noitemsep}
\newlist{remlist}{enumerate}{1}
\setlist[remlist]{label=(\arabic*), ref=\thethm (\arabic*),noitemsep}
\newlist{thmasslist}{enumerate}{1}
\setlist[thmasslist]{label=(\alph*),ref=(\alph*),noitemsep}
\crefname{thmasslisti}{Voraussetzung}{Voraussetzungen}

\newtheoremstyle{break}{}{}{}{}{\bfseries}{}{\newline}{}
\declaretheoremstyle[headpunct=.:]{claim}

\declaretheorem[
name=Satz,
%refname={theorem,theorems},        %Lower Case Versions of Theorem Type
% Refname={Satz,Sätze},
style=definition,
numberwithin=section]{thm}

\declaretheorem[
name=Korollar,
%refname={theorem,theorems},        %Lower Case Versions of Theorem Type
% Refname={Satz,Sätze},
style=definition,
sibling=thm]{cor}

\declaretheorem[
name=Satz,
%refname={theorem,theorems},        %Lower Case Versions of Theorem Type
% Refname={Satz,Sätze},
style=break,
sibling=thm]{thmbk}

\declaretheorem[
name=Lemma,
%refname={lemma,lemmas},
% Refname={Lemma,Lemmas},
style=definition,
sibling=thm]{lem}

\declaretheorem[
name=Definition,
%refname={lemma,lemmas},
% Refname={Definition,Definitionen},
style=definition,
sibling=thm]{defn}

\declaretheorem[
name=Bemerkung,
%refname={lemma,lemmas},
% Refname={Bemerkung,Bemerkungen},
style=definition,
sibling=thm]{rem}

\declaretheorem[
name=Beispiel,
%refname={lemma,lemmas},
% Refname={Bemerkung,Bemerkungen},
style=definition,
sibling=thm]{exmp}

\declaretheorem[
name=Beispiel,
%refname={theorem,theorems},        %Lower Case Versions of Theorem Type
% Refname={Satz,Sätze},
style=break,
sibling=thm]{exmpbk}

\declaretheorem[
name=Beh,
numbered=no,
%refname={lemma,lemmas},
% Refname={Bemerkung,Bemerkungen},
style=claim]{claim}

\declaretheorem[
name=Beweis,
numbered=no,
%refname={lemma,lemmas},
% Refname={Bemerkung,Bemerkungen},
qed=\ensuremath{\Diamond},
style=remark]{dproof}

% \Crefname{thm}{Satz}{Sätze}
% \Crefname{lem}{Lemma}{Lemmas}

\addtotheorempostheadhook[thm]{\crefalias{thmlisti}{thm}}
\addtotheorempostheadhook[lem]{\crefalias{thmlisti}{lem}}
\addtotheorempostheadhook[rem]{\crefalias{remlisti}{rem}}

% \theoremstyle{definition}
% \newtheorem{thm}{Satz}[section]
% \newtheorem{lem}[thm]{Lemma}
% \newtheorem{prop}[thm]{Proposition}
% \newtheorem{cor}[thm]{Korollar}

% \newtheorem{defn}[thm]{Definition}
% \newtheorem{conj}[thm]{Vermutung}
% \newtheorem{exmp}[thm]{Beispiel}

% % \theoremstyle{remark}
% \newtheorem{rem}[thm]{Bemerkung}
% \newtheorem*{note}{Merke}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\newcommand{\fracnorm}[3]{%
    \ensuremath{\frac{\! #1\!}{\! {}_{\phantom{#3}}\norm{#2}_{#3}\!}}%
}
\newcommand\coloniff{\vcentcolon\!\iff}

\makeatletter
\newcommand*{\transpose}{%
  {\mathpalette\@transpose{}}%
}
\newcommand*{\@transpose}[2]{%
  % #1: math style
  % #2: unused
  \raisebox{\depth}{$\m@th#1\intercal$}%
}
\makeatother

\newcommand{\stcomp}[1]{{#1}^{\mathsf{c}}} % Mengenkomplement
\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\KK}{\mathbb{K}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\OO}{\mathbb{O}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\bb}{\mathcal{B}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\ff}{\mathcal{F}}
\renewcommand{\gg}{\mathcal{G}}
\renewcommand{\ll}{\mathcal{L}}
\newcommand{\nn}{\mathcal{N}}
\newcommand{\vv}{\mathcal{V}}
\newcommand{\zz}{\mathcal{Z}}
\newcommand{\zspace}{V}
\newcommand{\zsigma}{\vv}
\newcommand{\interv}{\RR}
\newcommand{\indic}{\mathbbm{1}}
\newcommand{\indics}[1]{\mathbbm{1}_{\{#1\}}}

\newcommand{\Cb}[1]{C_b(#1)}
\newcommand{\expect}[1]{\EE[#1]}
\newcommand{\condexp}[2]{\EE[#1\,|\,#2]}
\newcommand{\dvar}[1]{\,\mathrm{d}#1}
\newcommand{\dmeas}[2]{\,#1(\mathrm{d}#2)}
\newcommand{\Var}[1]{\mathrm{Var}[#1]}
% \newcommand{\meas}[2]{\,\mathrm{d}#1(#2)}
\DeclarePairedDelimiter{\sprod}{\langle}{\rangle}	% spitze Klammern
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}		% Betrag
\DeclareMathOperator{\ggT}{ggT}

\newcommand\phantomarrow[2]{%
  \setbox0=\hbox{$\displaystyle #1\to$}%
  \hbox to \wd0{%
    $#2\mapstochar
     \cleaders\hbox{$\mkern-1mu\relbar\mkern-3mu$}\hfill
     \mkern-7mu\rightarrow$}%
  \,}

\begin{document}

\title{Konvergenzraten des Robbins-Monro-Algorithmus}
\author{Alexander Schlüter}
% \thanks{Dozent: Prof.~Dr.~Dereich, Betreuung: Johannes Blank}
\date{13. Juli 2016}
% Bachelorseminar Markovketten, WS 15/16
% {\let\newpage\relax\maketitle}
\maketitle
\begin{abstract}
  Der Robbins-Monro-Algorithmus ist ein rekursives Verfahren zur Bestimmung von
  Nullstellen einer Funktion, deren Wert nur stochastisch gestört gemessen
  werden kann. Im letzten Vortrag\footcite{seb} wurde die fast sichere Konvergenz gegen eine
  Nullstelle gezeigt. Ziel dieses Vortrages ist es, einen zentralen
  Grenzwertsatz für den Algorithmus zu beweisen, der Aufschluss über die
  Konvergenzgeschwindigkeit und die Form der Verteilung im Limes gibt.
\end{abstract}
% \newpage
\tableofcontents
% \newpage

\section{Erinnerungen}
Diese Arbeit orientiert sich an \cite[Kapitel 11.1]{lusch}. Im Folgenden sei    
\begin{itemize}
\item $(\Omega, \ff, P)$ ein Wahrscheinlichkeitsraum und $(\zspace,\zsigma)$ ein messbarer Raum,
\item $(Z_n)_{n\geq 1}$ eine Folge u.i.v $(\zspace, \zsigma)$-wertiger Zufallsvariablen,
% \item $I\subset\RR$ ein abgeschlossenes Intervall oder $I=\RR$, 
% \item $X_0$ eine von $(Z_n)_{n\geq 1}$ unabhängige $I$-wertige Zufallsvariable,
\item $X_0$ eine von $(Z_n)_{n\geq 1}$ unabhängige reelle Zufallsvariable,
\item $\ff_n=\sigma(X_0,Z_1,\dotsc,Z_n)$ und $\FF=(\ff_n)_{n\geq 0}$ eine Filtration,
\item $(\gamma_n)_{n\geq 1}$ eine Folge in $(0,\infty)$ und
\item $H:(\interv\times \zspace,\bb(\interv)\otimes\zsigma)\to(\RR,\bb(\RR))$ eine messbare
  Abbildung mit
  \begin{equation*}
    H(x,Z_1)\in\ll^1\quad\text{für alle $x\in \interv$.}
  \end{equation*}
\end{itemize}

Der \textbf{Robbins-Monro-Algorithmus} ist der $\FF$-adaptierte reelle Prozess
$X=(X_n)_{n\geq 0}$ definiert durch die Rekursion
\begin{equation}
  \label{eq:algo}
  X_{n+1}=X_n+\gamma_{n+1}H(X_n,Z_{n+1})\,.
\end{equation}
% wobei wir annehmen, dass die Werte immer in $I$ liegen.

Die \textbf{Erwartungswertfunktion} des Algorithmus ist definiert durch
\begin{equation*}
 h:\interv\to\RR,\quad h(x)\coloneqq\expect{H(x,Z_1)}
\end{equation*}
und außerdem sei
\begin{equation*}
 g:\interv\to\overline{\RR}_+,\quad g(x)\coloneqq\expect{H(x,Z_1)^2}.
\end{equation*}

\begin{rem}
  \label{rem:11.2}
  Wir haben gesehen, dass der Robbins-Monro-Algorithmus unter zusätzlichen
  Annahmen fast sicher gegen eine Nullstelle von $h$ konvergiert. Diese Annahmen
  waren
  \begin{enumerate}[label=(\alph*)]
  \item (Downcrossing-Bedingung) $h$ ist stetig und es gibt ein $x_0\in
    \set{h=0}$ mit
    \begin{equation*}
      \sup_{x\in\interv}(x-x_0)h(x)\leq 0\,,
    \end{equation*}
  \item (Beschränkte Störung) $X_0\in\ll^2$ und $g(x)\leq C(1+x^2)$ für eine
    Konstante $C\in\RR_+$ und alle $x\in\interv$,\label{rem:beschr}
  \item (Abnehmende Schrittweiten) $\sum_{n=1}^\infty \gamma_n=\infty$ und
    $\sum_{n=1}^\infty\gamma_n^2<\infty$.
  \end{enumerate}
\end{rem}
Wir benötigen noch folgende Resultate vom letzten Mal:
\begin{lem}[Substitution bei Unabhängigkeit]\label{lem:subst}
 Für $n\geq 1$ und jede messbare Funktion $f:(\interv\times \zspace,\bb(\interv)\otimes\zsigma)\to(\RR,\bb(\RR))$ mit
 wohldefiniertem Integral bezüglich $P^{(X_{n-1},Z_n)}$ gilt
 \begin{equation*}
  \condexp{f(X_{n-1}, Z_n)}{\ff_{n-1}}(\cdot)=\int f(X(\cdot), z)\dvar{P^{Z_1}(z)}\quad\text{fast sicher.}
 \end{equation*}
\end{lem}
\begin{proof}
 $Z_n$  ist unabhängig von $\ff_{n-1}$, also ist die Aussage vom letzten Mal anwendbar.
\end{proof}
\begin{lem}[Doob-Zerlegung]\label{lem:doob}
 Unter den o.g. Bedingungen ist $X$ ein $\ll^2$-Prozess und für die
 Doob-Zerlegung $X=N+A$ in ein Martinal $N=(N_n)_{n\geq 0}$ und einen
 previsiblen Prozess $A=(A_n)_{n\geq 0}$ gilt
 \begin{align*}
  N_n&=X_0+\sum_{j=1}^n\gamma_j(H(X_{j-1},Z_j)-h(X_{j-1})) \\
  \sprod{N}_n&=\sum_{j=1}^n\gamma_j^2(g(X_{j-1})-h(X_{j-1})^2)
 \end{align*}
 für $n\geq 0$. Außerdem ist $N$ ein $\ll^2$-Martingal.
\end{lem}
\section{Vorbereitungen}
Für die Untersuchung der Konvergenzraten brauchen wir eine Version des
zentralen Grenzwertsatzes für Martingale. Da dieser die Folgerung einer
stärkeren Konvergenz als Verteilungskonvergenz zulässt, lohnt es sich, diese
zunächst einzuführen.
\begin{defn}
  % Es sei $K$ ein Markov-Kern und $(K_n)_{n\geq 1}$ eine Folge von Markov-Kernen von
  % $(\Omega, \ff)$ nach $(\RR^d, \bb(\RR^d))$. Außerdem sei $(X_n)_{n\geq 1}$
  % eine Folge von Zufallsvariablen von $(\Omega, \ff)$ nach $(\RR^d, \bb(\RR^d))$.
  % \begin{thmlist}
  % \item Die Folge $(K_n)_{n\geq 1}$ heißt \textbf{schwach konvergent} gegen $K$ und wir
  %   schreiben
  %   \begin{equation*}
  %     K_n\overset{w}{\longrightarrow} K\, ,
  %   \end{equation*}
  %   falls für alle $f\in\Li{P}$ und $h\in\Cb{\RR^d}$ gilt
  %   \begin{equation*}
  %    \lim_{n\to\infty}\int f(\omega)h(x)\, P\otimes K_n(\mathrm{d}(\omega,x)) = \int f(\omega)h(x)\, P\otimes K(\mathrm{d}(\omega,x))\, .
  %   \end{equation*}
  %   \item Die Folge $(X_n)_{n\geq 1}$ heißt \textbf{stabil konvergent} gegen $K$
  %     und wir schreiben
  %     \begin{equation*}
  %       X_n\longrightarrow K
  %     \end{equation*}
  % \end{thmlist}
  Es sei $(Y_n)_{n\geq 1}$ eine Folge von Zufallsvariablen von $(\Omega, \ff)$
  nach $(\RR^d, \bb(\RR^d))$ und $\nu$ ein Wahrscheinlichkeitsmaß auf
  $\bb(\RR^d)$. $(Y_n)_{n\geq 1}$ heißt \textbf{mischend konvergent} gegen $\nu$
  und wir schreiben
  \begin{equation*}
    Y_n\longrightarrow\nu \quad\textit{mischend}, 
  \end{equation*}
  falls für alle $f\in\ll^1(P)$ und $h\in\Cb{\RR^d}$ gilt
  \begin{equation*}
   \lim_{n\to\infty} \expect{f\cdot h(Y_n)} = \int f\dvar{P}\int h\dvar{\nu}\,.
  \end{equation*}
\end{defn}
\begin{rem}
 Durch Wahl von $f=1$ folgt aus der mischenden Konvergenz die bekannte
 Konvergenz in Verteilung. 
\end{rem}
\begin{lem}\label{lem:misch}
  Sei $(Y_n)_{n\geq 1}$ eine Folge von reellen Zufallsvariablen und $\nu$ ein
  Wahrscheinlichkeitsmaß auf $(\RR,\bb(\RR))$ mit $Y_n\to\nu$ \textit{mischend}
  für $n\to\infty$. Außerdem sei $(U_n)_{n\geq 1}$ eine Folge von reellen
  Zufallsvariablen mit $U_n\to 0$ \textit{in Wahrscheinlichkeit}. Dann gilt für $n\to\infty$
\begin{equation*}
 (Y_n,U_n)\to\nu\otimes\delta_0\quad\textit{mischend.}
\end{equation*}
\end{lem}
\begin{proof}
 Siehe \cite[Korollar~5.29(a)]{lusch}.
\end{proof}
\begin{cor}[Stabiler Slutzky]\label{cor:mischsum}
  In der Situation von \cref{lem:misch} gilt
  \begin{equation*}
\begin{aligned}
 Y_n + U_n&\to\nu\quad &&\text{\textit{mischend} und} \\
 Y_n\cdot U_n&\to\delta_0 \quad &&\textit{mischend.}
\end{aligned}
\end{equation*}
\end{cor}
\begin{proof}
  Es seien $f\in\ll^1(P)$ und $h\in\Cb{\RR}$ beliebig. Dann ist
  $h':\RR^2\to\RR$, $h'(y,u)\coloneqq h(y+u)$ in $\Cb{\RR^2}$. Nach
  \cref{lem:misch} gilt
\begin{equation*}
 (Y_n,U_n)\to\nu\otimes\delta_0\quad\textit{mischend.}
\end{equation*}
Nach Definition der mischenden Konvergenz folgt
\begin{align*}
  \expect{f\cdot h(Y_n+U_n)}=\expect{f\cdot h'(Y_n,U_n)}&\to\int f\dvar{P}\int h'(y,u) \dvar{\nu\otimes\delta_0(y,u)}\\
&=\int f\dvar{P}\int h(y+0) \dvar{\nu(y)}\,.
\end{align*}
Für die Multiplikation funktioniert der Beweis analog.
\end{proof}
\begin{thm}[Stabiler CLT]\label{thm:clt}
  Es sei $M=(M_n)_{n\geq 0}$ ein $\ll^2$-Martingal bezüglich der Filtration
  $\FF$ und $(\kappa_n)_{n\geq 0}$ eine Folge in $(0,\infty)$ mit $\kappa_n\to 0$. Die
  folgenden Bedingungen seien erfüllt:
 \begin{thmasslist}
 \item Es existiert eine Konstante $v\geq 0$ mit \label{clt:vora}
   \begin{equation*}
     \kappa_n^2\sprod{M}_n\longrightarrow v\quad\textit{in Wahrscheinlichkeit}
   \end{equation*}
   für $n\to\infty$,
 \item ($\FF$-bedingte Lyapunov-Bedingung) Es existiert ein $\delta >0$, sodass \label{clt:vorb}
   \begin{equation*}
    \kappa_n^{2+\delta} \sum_{j=1}^n\condexp{\abs{\Delta M_j}^{2+\delta}}{\ff_{j-1}}\to 0\quad\textit{in Wahrscheinlichkeit}
   \end{equation*}
für ${n\to\infty}$.
 \end{thmasslist}
Dann gilt für ${n\to\infty}$:
\begin{equation*}
 \kappa_nM_n\to\nn(0,v)\quad\textit{mischend.} 
\end{equation*}
\end{thm}
\begin{proof}
 Siehe \cite[Satz~5.31 und Bemerkung~5.32(a)]{lusch}.
\end{proof}
\begin{rem}
 Sei $(Y_n)_{n\geq 1}$ eine Folge u.i.v. Zufallsvariablen mit $Y_1\in\ll^2$. Man bekommt den
 klassischen zentralen Grenzwertsatz durch Wahl von 
 \begin{equation*}
  M_n\coloneqq \sum_{j=1}^n(Y_1-\expect{Y_1})\,,
 \end{equation*}
 $\kappa_n\coloneqq 1/\sqrt{n}$ und die von $M$ erzeugte Filtration. Um die
 Lyapunov-Bedingung zu zeigen benötigt man die zusätzliche Annahme
 $Y_1\in\ll^{2+\delta}$ für ein $\delta>0$. Der Satz bleibt jedoch wahr, wenn
 man die Lyapunov-Bedingung durch die schwächere Lindeberg-Bedingung ersetzt,
 mit der man ohne diese Annahme auskommt\footcite[Korollar 5.33]{lusch}.
\end{rem}

Außerdem brauchen wir noch ein paar Resultate aus der Analysis:
\begin{lem}[Diskrete Regel von l'Hospital]\label{lem:lhospital}
 Sei $(a_n)_{n\geq 1}$ eine Folge in $\RR$ und $(b_n)_{n\geq 1}$ eine Folge in $(0,\infty)$, sodass 
 \begin{thmasslist}
 \item 
     $\sum_{n=1}^\infty b_n=\infty$ und
 \item es existiert $c\in\RR$ mit $\lim_{n\to\infty}a_n/b_n=c$.
 \end{thmasslist}
 Dann gilt
 \begin{equation*}
  \lim_{n\to\infty} \frac{\sum_{j=1}^na_j}{\sum_{j=1}^nb_j}=c\,.
 \end{equation*}
\end{lem}
\begin{proof}
 Seien $\varepsilon >0$ und $n_0\in\NN$ mit $\abs{a_n/b_n-c}\leq\epsilon$
 für alle $n>n_0$. Dann gilt für $n>n_0$
 \begin{align*}
  \abs*{\frac{\sum_{j=1}^na_j}{\sum_{j=1}^nb_j}-c}&\leq\frac{\sum_{j=1}^nb_j\abs{a_j/b_j-c}}{\sum_{j=1}^nb_j} \\
&=\frac{\sum_{j=1}^{n_0}b_j\abs{a_j/b_j-c}}{\sum_{j=1}^nb_j}+\frac{\sum_{j=n_0+1}^nb_j\abs{a_j/b_j-c}}{\sum_{j=1}^nb_j} \\
&\leq\frac{\sum_{j=1}^{n_0}b_j\abs{a_j/b_j-c}}{\sum_{j=1}^nb_j}+\varepsilon\,.
 \end{align*}
Es folgt
\begin{equation*}
  \limsup_{n\to\infty} \abs*{\frac{\sum_{j=1}^na_j}{\sum_{j=1}^nb_j}-c}\leq\varepsilon
\end{equation*}
und da $\varepsilon$ beliebig war folgt die Behauptung.
\end{proof}
\begin{lem}[Kronecker, WT Lem. 4.28]\label{lem:kronecker}
 Sei $(a_n)_{n\geq 1}$ eine monoton wachsende Folge positiver Zahlen mit
 $\lim_{n\to\infty}a_n=\infty$ und sei $(b_n)_{n\geq 1}$ eine Folge reeller
 Zahlen. Konvergiert $\sum_{j=1}^\infty b_j/a_j$ in $\RR$, so folgt
 \begin{equation*}
  \lim_{n\to\infty} \frac{1}{a_n}\sum_{j=1}^nb_j=0\,.
 \end{equation*}
\end{lem}
\begin{lem}\label{lem:sim}
  \begin{thmlist}
  \item Für reelle Konstanten $b\geq 0$ und $a>-b-1$ existiert $L\in(0,\infty)$,
    sodass\label{lem:sim:prod}
    \begin{equation*}
      \prod_{j=1}^n\left( 1+\frac{a}{b+j} \right) \sim Ln^a\quad\text{für $n\to\infty$.} 
    \end{equation*}
  \item Für $b>-1$ gilt
    \begin{equation*}
      \sum_{j=1}^nj^b \sim \frac{n^{b+1}}{b+1}\quad\text{für $n\to\infty$} 
    \end{equation*}
und für $b=-1$\label{lem:sim:sum}
    \begin{equation*}
      \sum_{j=1}^nj^{-1} \sim \log n\quad\text{für $n\to\infty$.} 
    \end{equation*}
  \end{thmlist}
\end{lem}
\begin{proof}
  \begin{thmlist}
  \item
Die Gammafunktion hat die Eigenschaft 
\begin{equation*}
 \Gamma(t+1)=t\Gamma(t)
\end{equation*}
für eine positive reelle Zahl $t$. Deshalb gilt
\begin{equation*}
  \prod_{j=1}^n\left( 1+\frac{a}{b+j} \right) =\frac{\prod_{j=1}^n(a+b+j)}{\prod_{j=1}^n(b+j)}=
\frac{\Gamma(a+b+n+1)}{\Gamma(a+b+1)}\frac{\Gamma(b+1)}{\Gamma(b+n+1)} 
\end{equation*}
Wir identifizieren den von $n$ unabhängigen Faktor
\begin{equation*}
 L\coloneqq\frac{\Gamma(b+1)}{\Gamma(a+b+1)}
\end{equation*}
und untersuchen den übrigen Term. Stirlings Formel für die Gammafunktion
\begin{equation*}
 \Gamma(t)\sim\sqrt{2\pi} t^{t-1/2}e^{-t},\quad t\to\infty
\end{equation*}
liefert
\begin{align*}
 \frac{\Gamma(a+b+n+1)}{\Gamma(b+n+1)}&\sim (a+b+n+1)^a\left( \frac{a+b+n+1}{b+n+1} \right)^{b+n+1/2}e^{-a} \\
&\sim n^a\left( 1+\frac{a}{b+n+1} \right)^{b+n+1}e^{-a}\,. %\sim n^a
\end{align*}
Im letzten Schritt wurde genutzt, dass
\begin{equation*}
 \lim_{n\to\infty}\left( \frac{a+b+n+1}{n} \right)^a=1\quad\text{und}\quad\lim_{n\to\infty}\sqrt{\frac{b+n+1}{a+b+n+1}}=1\,.
\end{equation*}
Schließlich folgt mit
\begin{equation*}
 \lim_{n\to\infty}\left( 1+\frac{a}{b+n+1} \right)^{b+n+1}=e^a
\end{equation*}
die Behauptung.
\item Falls $-1<b\leq 0$, so ist die Abbildung $(0,\infty)\to\RR$, $x\mapsto
  x^b$ monoton fallend. Also gilt für $j\in\NN$
  \begin{equation*}
   (j+1)\leq\int_j^{j+1}x^b\dvar{x}\leq j^b
  \end{equation*}
  und deshalb
  \begin{equation*}
   \frac{(n+1)^{b+1}-1}{b+1}=\int_1^{n+1}x^b\dvar{x}\leq\sum_{j=1}^nj^b\leq\int_0^nx^b\dvar{x}=\frac{n^{b+1}}{b+1}\,.
  \end{equation*}
Die Behauptung folgt, da
\begin{equation*}
 \lim_{n\to\infty} \frac{(n+1)^{b+1}-1}{b+1}\cdot\frac{b+1}{n^{b+1}}=1\,.
\end{equation*}
Mit demselben Argument bekommt man den Fall $b=-1$:
\begin{equation*}
   \log(n+1)=\int_1^{n+1}x^b\dvar{x}\leq\sum_{j=1}^nj^b\leq 1+\int_1^nx^b\dvar{x}=1+\log(n)
\end{equation*}
und
\begin{equation*}
 \lim_{n\to\infty}\frac{\log(n+1)}{\log(n)} = 1 = \lim_{n\to\infty}\frac{1+\log(n)}{\log(n)}\,.
\end{equation*}
  Falls $b>0$, so ist $x\mapsto x^b$ monoton wachsend, also
  \begin{equation*}
   j^b\leq\int_j^{j+1}x^b\dvar{x}\leq (j+1)^b
  \end{equation*}
  und die Abschätzung ist analog zu oben.
  \end{thmlist}
\end{proof}
\section{Konvergenzraten}
Der aus der Vorlesung Wahrscheinlichkeitstheorie bekannte zentrale Grenzwertsatz
kann verstanden werden als eine Aussage über die Konvergenzrate im starken
Gesetz der großen Zahlen. Letzteres besagt für u.i.v. Zufallsvariablen
$Y_1,Y_2,\dotsc\in\ll^1$ mit $\mu =\expect{Y_1}$
\begin{equation*}
  \frac{1}{n}\sum_{j=1}^nY_j-\mu\overset{n\to\infty}{\longrightarrow} 0\quad\textit{f.s.} 
\end{equation*}
Der zentrale Grenzwertsatz beantwortet (im Fall endlicher Varianz
$\sigma^2=\Var{Y_1}>0$) die Frage
  \enquote{Wie schnell ist die Konvergenz gegen $\mu$?}.

Dazu gibt er eine Folge (nämlich $\sqrt{n}$) an, die genau schnell genug wächst,
um sich mit der Nullfolge bei Multiplikation auszubalancieren. Das Ergebnis ist die
bekannte schwache Konvergenz gegen eine Normalverteilung  
\begin{equation*}
 \sqrt{n}\left(\frac{1}{n}\sum_{j=1}^nY_j-\mu\right)\overset{d}{\to}\nn(0,\sigma^2)\,. 
\end{equation*}
Jede langsamer wachsende Folge als $\sqrt{n}$ würde gegen die Nullfolge
\enquote{verlieren} und man bekäme Konvergenz des Produktes gegen die
Dirac-Verteilung bei 0. Umgekehrt wäre jede schneller wachsende Folge
\enquote{zu schnell} und das Produkt würde nicht mehr schwach konvergieren. 

Statt dem Gesetz der großen Zahlen werden wir gleich für den
Robbins-Monro-Algorithmus annehmen
\begin{equation*}
 X_n-x_0\to 0\quad\textit{f.s.} 
\end{equation*}
und wir suchen wie oben eine passende Folge $(a_n)$ mit $a_n\to\infty$ und
\begin{equation*}
 a_n(X_n-x_0)\overset{d}{\to}\nn(0, ?)\,. 
\end{equation*}
\begin{thm}[Konvergenzraten, stabiler CLT]
 \label{thm:haupt}
 Seien $\gamma_n=C_1/(C_2+n)$ für $n\geq 1$ mit reellen Konstanten $C_1>0$,
 $C_2\geq 0$ und $x_0\in\set{h=0}$. Die folgenden Bedingungen seien erfüllt:
 \begin{thmasslist}
 \item $X_n\to x_0$ f.s. für $n\to\infty$,
 \item $X_0\in\ll^2$ und $g(x)\leq C(1+x^2)$ für eine Konstante $C\in\RR_+$ und
   alle $x\in \interv$,\label{thm:haupt:b}
 \item $g$ sei stetig in $x_0$, h sei differenzierbar in $x_0$, $h'(x_0)<0$ und\label{thm:haupt:c}
   \begin{equation*}
    h(x) = h'(x_0)(x-x_0) + \mathcal{O}((x-x_0)^2)\quad\text{für $x\to x_0$}\,,
   \end{equation*}
 \item Es existieren $\varepsilon,\delta >0$ sodass $H(x,Z_1)\in\ll^{2+\delta}$ für
   alle $x\in \interv$ und\label{thm:haupt:d} 
   \begin{equation*}
    \sup_{\abs{x-x_0}\leq\varepsilon}\expect{\abs{H(x,Z_1)}^{2+\delta}}<\infty\,.
   \end{equation*}
 \end{thmasslist}
 Dann gilt für $n\to\infty$:
 \begin{enumerate}[label=(\roman*)]
 \item Falls $\abs{h'(x_0)}C_1>1/2$:\label{thm:haupt:i}
   \begin{equation*}
    \sqrt{n}(X_n-x_0)\to\nn\left( 0,\frac{g(x_0)C_1^2}{2\abs{h'(x_0)}C_1-1} \right)\quad\textit{mischend,}
   \end{equation*}
 \item Falls $\abs{h'(x_0)}C_1=1/2$:
   \begin{equation*}
    \sqrt{\frac{n}{\log n}}(X_n-x_0)\to\nn\left( 0,g(x_0)C_1^2 \right)\quad\textit{mischend,}
   \end{equation*}
 \item Falls $\abs{h'(x_0)}C_1<1/2$: 
   \begin{equation*}
     n^{\abs{h'(x_0)}C_1}(X_n-x_0)\to\xi \quad\text{f.s.}
   \end{equation*}
   für eine reelle Zufallsvariable $\xi$, die von $X_0$ abhängt.
 \end{enumerate}
\end{thm}
\begin{rem}
  \begin{remlist}
  \item Die quadratische Abschätzung für den Restterm in \cref{thm:haupt:c} ist
    erfüllt, wenn $h$ in einem offenen Intervall $J$ um $x_0$ differenzierbar
    ist mit Lipschitz-stetiger Ableitung. Denn dann gilt nach dem
    Mittelwertsatz, dass für alle $x\in J$ ein $\eta$ zwischen $x$ und $x_0$
    existiert mit
    \begin{align*}
      \abs{h(x)-h'(x_0)(x-x_0)}&=\abs{h(x)-h(x_0)-h'(x_0)(x-x_0)} \\
                               &=\abs{h'(\eta)(x-x_0)-h'(x_0)(x-x_0)} \\
                               &=\abs{h'(\eta)-h'(x_0)}\abs{x-x_0} \\
                               &\leq L\abs{\eta-x_0}\abs{x-x_0}\leq L\abs{x-x_0}^2\,,
    \end{align*}
    wobei $L$ die Lipschitz-Konstante von $h'$ sei. Insbesondere ist die
    Voraussetzung erfüllt, wenn $h$ in einer Umgebung von $x_0$ zweimal stetig
    differenzierbar ist.
  \item Es gibt einen \enquote{Phasenübergang} bei $\abs{h'(x_0}C_1=1/2$. Die schnellste Konvergenzrate ist im Fall $\abs{h'(x_0}C_1>1/2$
  realisiert. Dies ist intuitiv gesprochen erfüllt, wenn $h$ um $x_0$ herum
  schnell fällt bzw. die Schrittweiten $\gamma_n$ groß sind. Da allerdings bei
  Anwendung des Algorithmus $x_0$ und damit auch die Ableitung $h'(x_0)$
  unbekannt sind, ist nicht klar, wie groß die Schrittweite gewählt werden muss,
  um die schnelle Konvergenzrate tatsächlich zu bekommen.
\item Der ausgeartete Fall $g(x_0)=0$ ist im Satz nicht ausgeschlosssen. Die
  Limesverteilungen sind dann Dirac-Verteilungen in 0.
  \end{remlist}
\end{rem}
\begin{proof}[Beweis von \cref{thm:haupt}]
  Hier wird nur ein Teil vom Fall $\abs{h'(x_0)}C_1>1/2$ bewiesen. Für
  den vollen Beweis siehe \cite[Satz~11.4]{lusch}.
  
Wie wir letzte Woche gesehen haben, folgt aus \cref{thm:haupt:b}, dass $X$ ein
$\ll^2$-Prozess ist. Sei
\begin{equation*}
 a\coloneqq -h'(x_0)=\abs{h'(x_0)}\,. 
\end{equation*}
Nach \ref{thm:haupt:c} gilt $a>0$. Man wähle $n_0\in\NN$ mit $a\gamma_{n_0}<1$ und
definiere für $n\geq 0$
\begin{equation*}
 \beta_n\coloneqq\prod_{j=n_0}^n(1-a\gamma_j) =\prod_{j=1}^{n-n_0+1}(1-a\gamma_{j+n_0-1})\,.
\end{equation*}
Insbesondere gilt also $\beta_0=\dotsb=\beta_{n_0-1}=1$ und nach \cref{lem:sim:prod}
\begin{equation}\label{eq:beta}
 \beta_n\sim Ln^{-aC_1}\,
\end{equation}
für $n\to\infty$ mit einer Konstanten $L\in(0,\infty)$. Wegen $\gamma_n\sim
C_1/n$ folgt
\begin{equation}
  \frac{\gamma_n}{\beta_n}\sim\frac{C_1}{L}n^{aC_1-1}\label{eq:gammabeta}
\end{equation}
Mit der Doob-Zerlegung $X=N+A$ aus \cref{lem:doob} bekommen wir
\begin{align}
  \sqrt{n}(X_n-x_0)&=\sqrt{n}\beta_n\left(X_0-x_0+\sum_{j=1}^n\frac{1}{\beta_j}(X_j - X_{j-1} + X_{j-1})-\frac{1}{\beta_{j-1}}X_{j-1}-\frac{x_0}{\beta_j}+\frac{x_0}{\beta_{j-1}}\right)\nonumber \\
  \begin{split}\label{eq:zerl}
&=\phantom{+}\sqrt{n}\beta_n(X_0-x_0)\\
&\phantom{=}+\sqrt{n}\beta_n\sum_{j=1}^n\frac{1}{\beta_j}\Delta N_j\\
&\phantom{=}+\sqrt{n}\beta_n\sum_{j=1}^n\left(\frac{1}{\beta_j}\Delta A_j+(X_{j-1}-x_0)(\frac{1}{\beta_j}-\frac{1}{\beta_{j-1}})\right)\,. 
\end{split}
\end{align}
Wir wollen die drei Summanden einzeln auf ihr Verhalten für $n\to\infty$
untersuchen. Definiere dazu für $n\geq 0$
\begin{align*}
 M_n&\coloneqq \sum_{j=1}^n\frac{1}{\beta_j}\Delta N_j\\
B_n&\coloneqq\sum_{j=1}^n\left(\frac{1}{\beta_j}\Delta A_j+(X_{j-1}-x_0)(\frac{1}{\beta_j}-\frac{1}{\beta_{j-1}})\right)\,.
\end{align*}
Da $\sqrt{n}\beta_n\sim Ln^{-aC_1+1/2}\to 0$ gilt auch
$\sqrt{n}\beta_n(X_0-x_0)\to 0$ punktweise.

Es gilt auch
$\sqrt{n}\beta_nB_n\to 0$ fast sicher, allerdings ist der Beweis nicht trivial
und kann nachgelesen werden in \cite[Satz~11.4]{lusch}. Er basiert auf der Idee,
die Zuwächse $\Delta B_n$ für große $n$ gegen $(X_n-x_0)^2$ abzuschätzen. Dazu
wird \cref{thm:haupt:c} genutzt.

Im Folgenden wird mit dem stabilen CLT~\ref{thm:clt} gezeigt, dass
\begin{equation}\label{eq:mterm}
 \sqrt{n}\beta_nM_n\to\nn\left( 0,\frac{g(x_0)C_1^2}{2aC_1-1} \right)\quad\textit{mischend.}
\end{equation}
Wähle also $\kappa_n=\sqrt{n}\beta_n$, dann gilt wie oben schon bemerkt
$\kappa_n\to 0$. $M=(M_n)_{n\geq 0}$ ist ein $\ll^2$-Martingal, da $N$ eins ist.

Wir wollen nun \cref{clt:vora} des CLT überprüfen. Es gilt nach der
Darstellung für $\sprod{N}$ aus \cref{lem:doob}
\begin{align*}
 \Delta\sprod{M}_n&=\condexp{(\Delta M_n)^2}{\ff_{n-1}}=\frac{1}{\beta_n^2}\condexp{(\Delta N_n)^2}{\ff_{n-1}}\\
&=\frac{\gamma_n^2}{\beta_n^2}(g(X_{n-1})-h(X_{n-1})^2)
\end{align*}
für $n\geq 1$. Wegen der Stetigkeit von $h$ und $g$ in $x_0$ gilt
\begin{equation*}
 g(X_{n-1})-h(X_{n-1})^2\to g(x_0)-h(x_0)^2=g(x_0)\quad\text{f.s.,}
\end{equation*}
und zusammen mit \cref{eq:gammabeta} ergibt sich
\begin{equation*}
 \frac{\Delta\sprod{M}_n}{n^{2aC_1-2}}\to\frac{g(x_0)C_1^2}{L^2}\quad\text{f.s.} 
\end{equation*}
für $n\to\infty$.
Da wir $aC_1>1/2$ annehmen, ist $2aC_1-2>-1$ und deshalb
\begin{equation*}
 \sum_{j=1}^\infty j^{2aC_1-2}=\infty\,. 
\end{equation*}
Somit können wir die diskrete Regel von l'Hospital~\ref{lem:lhospital} anwenden
und bekommen
\begin{equation*}
 \frac{\sprod{M}_n}{\sum_{j=1}^n j^{2aC_1-2}}\to\frac{g(x_0)C_1^2}{L^2}\quad\text{f.s.}
\end{equation*}
für $n\to\infty$. Aus \cref{lem:sim:sum} und \cref{eq:beta} kennen wir die
asymptotischen Äquivalenzen
\begin{equation*}
  \frac{1}{\sum_{j=1}^n j^{2aC_1-2}}\overset{\ref{lem:sim}}{\sim}\frac{2aC_1-1}{n^{2aC_1-1}}\overset{\eqref{eq:beta}}{\sim}\frac{2aC_1-1}{L^2}n\beta_n^2\
\end{equation*}
d.h. insgesamt
\begin{equation*}
 \kappa_n^2\sprod{M}_n\to\frac{g(x_0)C_1^2}{2aC_1-1}\quad\text{f.s.}
\end{equation*}

Zur Anwendung des CLT fehlt uns noch die bedingte Lyapunov-Bedingung. Es gilt
für $j\geq 1$
\begin{equation*}
 \Delta M_j=\frac{\gamma_j}{\beta_j}(H(X_{j-1},Z_j)-h(X_{j-1}))\,,
\end{equation*}
also mit dem $\delta$ aus \cref{thm:haupt:d}
\begin{equation}\label{eq:lyasup}
 \sum_{j=1}^n\condexp{\abs{\Delta M_j}^{2+\delta}}{\ff_{j-1}}\leq\sum_{j=1}^n\left( \frac{\gamma_j}{\beta_j} \right)^{2+\delta}\sup_{k\geq 1}\condexp{\abs{H(X_{k-1}, Z_k)-h(X_{k-1})}^{2+\delta}}{\ff_{k-1}}\,.
\end{equation}
Das Supremum über die bedingten Erwartungswerte kann gebildet werden, weil die
Indexmenge abzählbar ist. Für $n\to\infty$ gilt wegen $\gamma_n\sim C_1/n$
\begin{equation*}
 (\sqrt{n}\beta_n)^{2+\delta} \left( \frac{\gamma_n}{\beta_n} \right)^{2+\delta}=n^{1+\delta/2}\gamma_n^{2+\delta}\sim C_1^{2+\delta}n^{-1-\delta/2}
\end{equation*}
und deshalb
\begin{equation*}
 \sum_{n=1}^\infty \kappa_n^{2+\delta}\left( \frac{\gamma_n}{\beta_n} \right)^{2+\delta}<\infty\,.
\end{equation*}
Mit dem Kronecker-Lemma~\ref{lem:kronecker} folgt
\begin{equation*}
 \kappa_n^{2+\delta} \sum_{j=1}^n\left( \frac{\gamma_j}{\beta_j} \right)^{2+\delta}\to 0\,.
\end{equation*}
Es soll nun das Supremum in \cref{eq:lyasup} mithilfe von \cref{thm:haupt:d} abgeschätzt werden.
Mittels Substitution bei Unabhängigkeit \ref{lem:subst} gilt für $n\geq 1$
\begin{align*}
  \condexp{\abs{H(X_{n-1},Z_n)-h(X_{n-1})}^{2+\delta}}{\ff_{n-1}}&=\int\abs{H(X_{n-1},z)-h(X_{n-1})}^{2+\delta}\dvar{P^{Z_1}(z)} \\
&=\norm{H(X_{n-1},Z_1(\cdot))-h(X_{n-1})}_{2+\delta}^{2+\delta}
\end{align*}
mit der Norm auf $\ll^{2+\delta}(P)$. Definiere
$\varphi(x)\coloneqq\expect{\abs{H(x,Z_1)}^{2+\delta}}$. Durch die
Dreiecksungleichung und die Jensensche Ungleichung kann für $x\in \interv$ man abschätzen
 \begin{align*}
   \norm{H(x,Z_1)-h(x)}_{2+\delta}&\leq\norm{H(x,Z_1)}_{2+\delta}+\abs{h(x)}\\
    &=\varphi(x)^{1/(2+\delta)}+\abs{\expect{H(x, Z_1)}}\\
&\leq\varphi(x)^{1/(2+\delta)}+\expect{\abs{H(x, Z_1)}^{2+\delta}}^{1/(2+\delta)}\\
&=2\varphi(x)^{1/(2+\delta)}\,,
 \end{align*}
 also gilt
 \begin{equation*}
  \sup_{n\geq 1} \condexp{\abs{H(X_{n-1},Z_n)-h(X_{n-1})}^{2+\delta}}{\ff_{n-1}}\leq 2^{2+\delta}\sup_{n\geq 1}\varphi(X_{n-1})<\infty\,\text{f.s.}
 \end{equation*}
 Die rechte Seite ist fast sicher endlich nach \cref{thm:haupt:d} zusammen mit der fast sicheren Konvergenz von $X_n$
 gegen $x_0$.
 
Insgesamt folgt die Lyapunov-Bedingung
\begin{equation*}
  \kappa_n^{2+\delta} \sum_{j=1}^n\condexp{\abs{\Delta M_j}^{2+\delta}}{\ff_{j-1}}\to 0\quad\text{f.s.}
\end{equation*}
und der stabile CLT liefert
\begin{equation*}
 \sqrt{n}\beta_nM_n\to\nn\left( 0,\frac{g(x_0)C_1^2}{2aC_1-1} \right)\quad\textit{mischend.}
\end{equation*}
Wir kehren zur Zerlegung aus \cref{eq:zerl} zurück:
\begin{equation*}
 \sqrt{n}(X_n-x_0)=\sqrt{n}\beta_n(X_0-x_0) +\sqrt{n}\beta_nM_n+\sqrt{n}\beta_nB_n
\end{equation*}
Aufgrund der fast sicheren Konvergenz der beiden anderen Terme gegen $0$ folgt die Behauptung mittels \cref{cor:mischsum}.
\end{proof}
\begin{rem}[{Optimale Schrittweiten, \cite[S. 386]{lusch}}]
 Im nicht-ausgearteten Fall $g(x_0)>0$ wird die Limesvarianz
 \begin{equation*}
  \frac{g(x_0)C_1^2}{2\abs{h'(x_0)}C_1-1} 
 \end{equation*}
unter der Bedingung $\abs{h'(x_0)}C_1>1/2$ als Funktion von $C_1$ durch
\begin{equation*}
 C_1=\frac{1}{\abs{h'(x_0)}} 
\end{equation*}
minimiert mit resultierender Varianz
\begin{equation*}
 \frac{g(x_0)}{h'(x_0)^2}=\frac{\Var{H(x_0,Z_1)}}{h'(x_0)^2}\,. 
\end{equation*}
Die beste Wahl der Schrittweiten ist also
\begin{equation*}
 \gamma_n=\frac{1}{\abs{h'(x_0)}(C_2+n)} 
\end{equation*}
für $n\geq 1$. Wie oben schon bemerkt ist allerdings bei der Nullstellensuche
$h'(x_0)$ nicht bekannt, sodass der Algorithmus mit diesen Schrittweiten nicht
implementierbar ist.
\end{rem}
\begin{exmp}[{Rekursiver Quantilschätzer, \cite[Bsp. 11.7]{lusch}}]
  Die Verteilungsfunktion $F$ von $P^{Z_1}$ sei stetig und es sei $\alpha\in (0,
  1)$ gegeben. Durch den Robbins-Monro-Algorithmus kann rekursiv ein
  $\alpha$-Quantil von $P^{Z_1}$ geschätzt werden. Dazu sei $X_0=s\in\RR$,
  $H(x,z)=\alpha-\indics{z\leq x}$, $C_1>0$ und $C_2\geq 0$. Die
  Rekursionsgleichung hat dann die Form
  \begin{equation*}
   X_{n+1}=X_n+\frac{C_1}{C_2+n+1}(\alpha-\indics{Z_{n+1}\leq X_n})
  \end{equation*}
  und es gilt $h(x)=\alpha -F(x)$ und $g(x)=\alpha^2-2\alpha F(x)+F(x)$ für
  $x\in\RR$. Außerdem ist h monoton fallend und $\set{h=0}=\set{F=\alpha}$ die
  Menge der $\alpha$-Quantile von $P^{Z_1}$.
  
Nach dem Resultat von letzter Woche gilt
\begin{equation*}
 X_n\to X_\infty\quad\textit{f.s.}
\end{equation*}
für eine reelle Zufallsvariable $X_\infty$ mit
\begin{equation*}
  P(X_\infty\in\set{F=\alpha})=1\,. 
\end{equation*}
Gilt $\set{F=\alpha}=\{x_0\}$ und ist $F$ in einer Umgebung von $x_0$
differenzierbar mit Lipschitz-stetiger Ableitung und $F'(x_0)>0$, so ist
\cref{thm:haupt} anwendbar, denn die Voraussetzungen \ref{thm:haupt:b} und
\ref{thm:haupt:d} sind aufgrund der Beschränktheit von $H$ erfüllt. Bei
optimaler Wahl der Schrittweiten mit $C_1=1/\abs{h'(x_0)}=1/F'(x_0)$ folgt
\begin{equation*}
 \sqrt{n}(X_n-x_0)\to\nn\left(0, \frac{\alpha(1-\alpha)}{F'(x_0)^2}\right)\quad\textit{mischend}
\end{equation*}
für $n\to\infty$.
\end{exmp}
% \nocite{seb}
\printbibliography

\end{document}